# 13.3 - Floating-point Numbers, Representation and Manipulation

## Format of Binary Floating-Point Real Numbers

- Represented as M×2EM \times 2^EM×2E where:
    - **Mantissa (M):** Fractional part.
    - **Exponent (E):** Power of 2.
    - **Sign bit:** Indicates positive or negative.
- Binary point usually assumed between bits in mantissa.
- Example: 8 bits mantissa and 8 bits exponent stored.

## Conversion: Binary Floating-Point and Denary Numbers

- **Binary to Denary:** Sum mantissa fractions multiplied by 2exponent2^{\text{exponent}}2exponent.
- **Denary to Binary:** Convert to fraction form M×2EM \times 2^EM×2E, normalize mantissa between [0.1,1] or [1.0,-1.0] .
- Normalisation moves binary point and adjusts exponent accordingly.

## Normalising Floating-Point Numbers

- Shifting mantissa bits left or right to make mantissa start at 0.1 for positive or 1.0 for negative numbers.
- For each shift left, decrement exponent by 1; for shift right, increment exponent by 1.
- Improves precision of representation.

## Consequences of Binary Approximation of Real Numbers

- Binary floating-point can only approximate some real numbers.
- Accuracy improves with more mantissa bits; range increases with more exponent bits.
- Trade-off exists: Increasing mantissa bits reduces exponent bits and vice versa.

## Binary Representation Rounding Errors

- Overflow: When a value exceeds maximum representable size.
- Underflow: When a value is too small for representation.
- Inability to store exact zero in normalized floating-point (mantissa must not be zero).
- Rounding errors occur due to finite mantissa length; calculations can accumulate small errors.